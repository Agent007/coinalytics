{"metadata":{"name":"KafkaDirect","user_save_timestamp":"1970-01-01T00:00:00.000Z","auto_save_timestamp":"1970-01-01T00:00:00.000Z","language_info":{"name":"scala","file_extension":"scala","codemirror_mode":"text/x-scala"},"trusted":true,"customLocalRepo":null,"customRepos":null,"customDeps":null,"customImports":null,"customSparkConf":null},"cells":[{"metadata":{},"cell_type":"markdown","source":"# Demo: Real-Time Chart via Direct Kafka Spark Streaming Connection"},{"metadata":{},"cell_type":"markdown","source":"Designate local repo to cache Spark Streaming libraries:"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"# :local-repo /tmp/repo","outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":":dp org.apache.spark % spark-streaming_2.10 % 1.4.1\norg.apache.spark % spark-streaming-kafka_2.10 % 1.4.1\n- org.apache.spark % spark-core_2.10 % _\n- org.apache.hadoop % _ % _","outputs":[{"name":"stdout","output_type":"stream","text":"warning: there were 4 feature warning(s); re-run with -feature for details\nglobalScope.jars: Array[String] = [Ljava.lang.String;@3cc95829\nres5: List[String] = List(/tmp/repo/cache/org.apache.spark/spark-streaming-kafka_2.10/jars/spark-streaming-kafka_2.10-1.4.1.jar, /tmp/repo/cache/org.slf4j/slf4j-api/jars/slf4j-api-1.7.6.jar, /tmp/repo/cache/log4j/log4j/jars/log4j-1.2.14.jar, /tmp/repo/cache/org.xerial.snappy/snappy-java/bundles/snappy-java-1.1.1.6.jar, /tmp/repo/cache/com.101tec/zkclient/jars/zkclient-0.3.jar, /tmp/repo/cache/org.apache.kafka/kafka_2.10/jars/kafka_2.10-0.8.2.1.jar, /tmp/repo/cache/org.spark-project.spark/unused/jars/unused-1.0.0.jar, /tmp/repo/cache/org.apache.kafka/kafka-clients/jars/kafka-clients-0.8.2.1.jar, /tmp/repo/cache/com.yammer.metrics/metrics-core/jars/metrics-core-2.2.0.jar, /tmp/repo/cache/net.jpountz.lz4/lz4/jars/lz4-1.2.0.jar,..."},{"metadata":{},"data":{"text/html":"<div class=\"container-fluid\"><div><div class=\"col-md-12\"><div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anonc8a433147b768288da18838dc3423f94&quot;,&quot;dataInit&quot;:[{&quot;string value&quot;:&quot;/tmp/repo/cache/org.apache.spark/spark-streaming-kafka_2.10/jars/spark-streaming-kafka_2.10-1.4.1.jar&quot;},{&quot;string value&quot;:&quot;/tmp/repo/cache/org.slf4j/slf4j-api/jars/slf4j-api-1.7.6.jar&quot;},{&quot;string value&quot;:&quot;/tmp/repo/cache/log4j/log4j/jars/log4j-1.2.14.jar&quot;},{&quot;string value&quot;:&quot;/tmp/repo/cache/org.xerial.snappy/snappy-java/bundles/snappy-java-1.1.1.6.jar&quot;},{&quot;string value&quot;:&quot;/tmp/repo/cache/com.101tec/zkclient/jars/zkclient-0.3.jar&quot;},{&quot;string value&quot;:&quot;/tmp/repo/cache/org.apache.kafka/kafka_2.10/jars/kafka_2.10-0.8.2.1.jar&quot;},{&quot;string value&quot;:&quot;/tmp/repo/cache/org.spark-project.spark/unused/jars/unused-1.0.0.jar&quot;},{&quot;string value&quot;:&quot;/tmp/repo/cache/org.apache.kafka/kafka-clients/jars/kafka-clients-0.8.2.1.jar&quot;},{&quot;string value&quot;:&quot;/tmp/repo/cache/com.yammer.metrics/metrics-core/jars/metrics-core-2.2.0.jar&quot;},{&quot;string value&quot;:&quot;/tmp/repo/cache/net.jpountz.lz4/lz4/jars/lz4-1.2.0.jar&quot;},{&quot;string value&quot;:&quot;/tmp/repo/cache/org.scala-lang/scala-library/jars/scala-library-2.10.4.jar&quot;},{&quot;string value&quot;:&quot;/tmp/repo/cache/org.apache.spark/spark-streaming_2.10/jars/spark-streaming_2.10-1.4.1.jar&quot;},{&quot;string value&quot;:&quot;file:/opt/docker/lib/common.common-0.6.0-scala-2.10.4-spark-1.4.1-hadoop-2.7.1-with-hive-with-parquet.jar&quot;}],&quot;genId&quot;:&quot;654167233&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/tableChart'], \n      function(playground, _magictableChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magictableChart,\n    \"o\": {\"headers\":[\"string value\"],\"nrow\":13,\"shown\":13,\"width\":600,\"height\":400}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    </div></div></div></div>"},"output_type":"execute_result","execution_count":2}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"import kafka.serializer.StringDecoder\n\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.streaming.kafka._\n\nval ssc = new StreamingContext(sparkContext, Seconds(1))\nval topics = \"transactions\"\nval brokers = \"kafka:9092\"\nval topicsSet = topics.split(\",\").toSet\nval kafkaParams = Map[String, String](\"metadata.broker.list\" -> brokers)\nval messages = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, topicsSet)\n\n// Get the lines, split them into words, count the words and print\nval lines = messages.map(_._2)\nval words = lines.flatMap(_.split(\" \"))\nval wordCounts = words.map(x => (x, 1L)).reduceByKey(_ + _)\nwordCounts.print()","outputs":[{"name":"stdout","output_type":"stream","text":"import kafka.serializer.StringDecoder\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.streaming.kafka._\nssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@68456311\ntopics: String = transactions\nbrokers: String = kafka:9092\ntopicsSet: scala.collection.immutable.Set[String] = Set(transactions)\nkafkaParams: scala.collection.immutable.Map[String,String] = Map(metadata.broker.list -> kafka:9092)\nmessages: org.apache.spark.streaming.dstream.InputDStream[(String, String)] = org.apache.spark.streaming.kafka.DirectKafkaInputDStream@236b2692\nlines: org.apache.spark.streaming.dstream.DStream[String] = org.apache.spark.streaming.dstream.MappedDStream@4fb14d68\nwords: org.apache.sp..."},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":3}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"import notebook._, front._, widgets._\nimport notebook.JsonCodec._\nimport play.api.libs.json._\nimport notebook.Series,Series._\n\ndef series(values:Seq[(Double, Double)]) = Seq(Series(\"Fun-DStream\", \"#4a6860\", values))\nval p = new Playground(series(Nil), List(Script(\"rickshawts\", \n                                         Json.obj(\n                                           \"renderer\" -> \"stack\",\n                                           \"fixed\" -> Json.obj(\n                                              (\"interval\" -> 1000),\n                                              (\"max\" -> 5),\n                                              (\"baseInSec\" -> 1)\n                                         )))))(seriesCodec)\nwordCounts.foreachRDD { rdd => \n  val count = rdd.count\n  p(series(Seq((System.currentTimeMillis.toDouble, count.toDouble))))\n}\nssc.start\np","outputs":[{"name":"stdout","output_type":"stream","text":"import notebook._\nimport front._\nimport widgets._\nimport notebook.JsonCodec._\nimport play.api.libs.json._\nimport notebook.Series\nimport Series._\nseries: (values: Seq[(Double, Double)])Seq[notebook.Series]\np: notebook.front.Playground[notebook.Series] = <Playground widget>\nres7: notebook.front.Playground[notebook.Series] = <Playground widget>\n"},{"metadata":{},"data":{"text/html":"<div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon62a0f00335564026f0f90b72bcbb6fb5&quot;,&quot;dataInit&quot;:[{&quot;name&quot;:&quot;Fun-DStream&quot;,&quot;color&quot;:&quot;#4a6860&quot;,&quot;data&quot;:[]}],&quot;genId&quot;:&quot;1028563036&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/rickshawts'], \n      function(playground, _rickshawts) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _rickshawts,\n    \"o\": {\"renderer\":\"stack\",\"fixed\":{\"interval\":1000,\"max\":5,\"baseInSec\":1}}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    </div>"},"output_type":"execute_result","execution_count":4}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"ssc.stop()","outputs":[{"name":"stdout","output_type":"stream","text":"-------------------------------------------\nTime: 1439271724000 ms\n-------------------------------------------\n(\"size\":258,\n,1)\n(\"value\":116740479,\n,1)\n(,945)\n(\"spent\":true,\n,3)\n(}\n},2)\n(\"script\":\"76a914da5dde8abec4f3b67561bcd06aaf28b790cff75588ac\"\n,1)\n(\"tx_index\":97856706,\n,1)\n(\"addr_tag\":\"LuckyBit,2)\n(\"addr\":\"1LuckyR1fFHEsXYyx5QK4UFzv3PEAepPMK\",\n,1)\n(\"addr\":\"13XHUVrPPZTZFmNUGBgo9SY2eivzpYtfsk\",\n,1)\n...\n\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":6}]}],"nbformat":4}